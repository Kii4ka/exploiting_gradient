# Code of the paper: Learning to Active Learn by Gradient Variation based on Instance Importance

https://www.sciencedirect.com/science/article/pii/S0957417424001854?via%3Dihub


---

# Exploiting Gradient: Usage Guide of the `main.py` script

This project includes a script `main.py` that is used for active learning with various techniques. Below is the required usage format and a breakdown of the parameters.

## Usage

```bash
python src/main.py <samples> <training_epochs> <active_epochs> <reproducibility> <use_features> <resume> <limit_samples> <debug> <technique>
```

### Parameters

- `<samples>`: **(int)** Number of samples to select.
- `<training_epochs>`: **(int)** Number of training epochs.
- `<active_epochs>`: **(int)** Number of active learning epochs.
- `<reproducibility>`: **(1 or 0)** Whether to enable reproducibility (1 for true, 0 for false).
- `<use_features>`: **(1 or 0)** Whether to use specific features (1 for yes, 0 for no).
- `<resume>`: **(1 or 0)** Whether to resume training from a checkpoint (1 for yes, 0 for no).
- `<limit_samples>`: **(int)** Set a limit on the number of samples (provide a number, or use 0 for no limit).
- `<debug>`: **(1 or 0)** Whether to run in debug mode (1 for true, 0 for false).
- `<technique>`: **(int)** Selection technique for active learning:
  - 1: SCORE SINGLE DISTANCE
  - 2: SCORE OTHER DISTANCE
  - 3: CLASS SINGLE DISTANCE
  - 4: CLASS OTHER DISTANCE
  - 5: RANDOM
  - 6: LEAST CONFIDENCE (UNCERTAINTY SAMPLING)

### Example

To run the script with 100 samples, 10 training epochs, 5 active learning epochs, reproducibility enabled, using features, no resuming, no sample limit, debug mode off, and using LEAST CONFIDENCE technique:

```bash
python src/main.py 100 10 5 1 1 0 0 0 6
```

---
