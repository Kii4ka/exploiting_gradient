2,074 / 5,000
# calculating correlations
# correlations = []
# for index in range(len(just_annotated_x)): # I know that I can do this in the previous for, but now I want a clearer code rather than optimized
# correlations.append(scipy.stats.kendalltau(single_distances[index], other_distances[index]))

# ALSO CALCULATE THE GRADIENT OF THE NETWORK WITH RESPECT TO x,y minus x[index],y[index]
# THE CLOSER THIS GRADIENT IS TO THE TRUE GRADIENT, THE LESS USEFUL THE EXAMPLE WAS

# LET'S ADD THE DISTANCES FOR THE VARIOUS EPOCHS, PERHAPS KEEPING OTHER STATISTICS IN THEM
# MAX, MIN,

# -------------------------------------------------------------------------------------------------------------------

# IN THE ALGORTHM WRITTEN BY FRANCESCO IT IS CALCULATED THE SCORE FOR ALL THE EXAMPLES
# IN REALITY YOU COULD (IT IS NECESSARY FOR THE SECOND DISTANCE) LIMIT THE CALCULATION OF THE SCORE
# TO THE "NEW" EXAMPLES (THOSE JUST NOTED BY THE EXPERT OR AT THE BEGINNING YOU MAKE A RANDOM CHOICE)
# BE CAREFUL BY LIMITING THE TECHNIQUE TO A SET OF EXAMPLES WE WILL HAVE A SMALL NUMBER OF
# EXAMPLES ON WHICH TO TRAIN THE PREDICTOR

# AT THE END WE MUST ASSIGN A SCORE TO THE EXAMPLES
# I TAKE THE SUMS AND USE AS SCORE (distance between the training gradient and):
# ONLY THE FIRST DISTANCE (the gradient with respect to the single example)
# ONLY THE SECOND DISTANCE (the gradient with respect to all except the single example)
# THE LINEAR COMBINATION OF THE TWO (THE SECOND SHOULD BE REVERSED)

# STATISTICAL CORRELATION (PEARSON/KENDALL) BETWEEN FIRST AND SECOND DISTANCE - (IMPLICATION?)

# LEARN A PREDICTOR THAT ESTIMATE US FOR NEW EXAMPLES:
# (I WOULD TRY A PREDICTOR THAT USES SOME INTERMEDIATE FEATURES OF THE NETWORK AND ANOTHER THAT USES THE INITIALS)
# THE SCORE
# THE PERCENTILE OF THE EXAMPLE (DIVE INTO BANDS - USEFUL, MEDIUM, USELESS)

# TRY TO SELECT THE NEW EXAMPLES FOR WHICH TO REQUEST ANNOTATION
# 1) COMPARED TO THE SCORE/PERCENTILE
# 2) COMPARED TO THE COMBINATION OF THE SCORE/PERCENTILE AND THE UNCERTAINTY

# COMPARISON WITH THE SELECTION RANDOM
# UNCERTAINTY-BASED SELECTION COMPARISON
Send feedback
